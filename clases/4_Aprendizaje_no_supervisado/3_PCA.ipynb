{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinaMariaCastro/curso-ia-para-economia/blob/main/clases/4_Aprendizaje_no_supervisado/3_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "DHNQuIqAmvl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inteligencia Artificial con Aplicaciones en Econom√≠a I**\n",
        "\n",
        "- üë©‚Äçüè´ **Profesora:** [Lina Mar√≠a Castro](https://www.linkedin.com/in/lina-maria-castro)  \n",
        "- üìß **Email:** [lmcastroco@gmail.com](mailto:lmcastroco@gmail.com)  \n",
        "- üéì **Universidad:** Universidad Externado de Colombia - Facultad de Econom√≠a"
      ],
      "metadata": {
        "id": "5iiwfbXCQv5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîΩ **Reducci√≥n de Dimensionalidad con An√°lisis de Componentes Principales (PCA)**"
      ],
      "metadata": {
        "id": "dU-UqrDscLfY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiqSyj3lYBPG"
      },
      "source": [
        "**Objetivos de Aprendizaje**\n",
        "\n",
        "Al finalizar este notebook, ser√°s capaz de:\n",
        "\n",
        "1.  **Entender la \"Maldici√≥n de la Dimensionalidad\"** y por qu√© reducir el n√∫mero de variables es crucial en el an√°lisis econ√≥mico y de negocios.\n",
        "2.  **Aplicar el An√°lisis de Componentes Principales (PCA)** para transformar un conjunto de datos con muchas variables correlacionadas en un conjunto m√°s peque√±o de variables no correlacionadas (componentes).\n",
        "3.  **Interpretar los resultados de PCA** en un contexto de negocio, traduciendo los componentes principales a conceptos econ√≥micos tangibles como \"√≠ndices sint√©ticos\".\n",
        "4.  **Utilizar PCA como un paso de preprocesamiento** para mejorar el rendimiento de modelos de clustering (K-Means) y de aprendizaje supervisado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRi4wWjEYBPI"
      },
      "source": [
        "**Introducci√≥n**\n",
        "\n",
        "Imaginemos que somos analistas en el Banco Interamericano de Desarrollo y se nos encarga la tarea de crear un √∫nico **\"√çndice de Salud Macroecon√≥mica\"** para todos los pa√≠ses de Am√©rica Latina. Contamos con una gran cantidad de datos: PIB per c√°pita, tasa de inflaci√≥n, tasa de desempleo, balanza comercial, inversi√≥n extranjera directa, deuda p√∫blica, etc. (m√°s de 50 indicadores).\n",
        "\n",
        "El problema es evidente: ¬øc√≥mo comparamos a Colombia con Argentina si uno tiene baja inflaci√≥n pero alto desempleo, y el otro tiene casi pleno empleo pero una inflaci√≥n desbordada? Comparar pa√≠ses a trav√©s de 50 dimensiones es computacionalmente complejo y, sobre todo, dif√≠cil de interpretar de manera intuitiva. Adem√°s, muchas de estas variables est√°n correlacionadas.\n",
        "\n",
        "La **reducci√≥n de dimensionalidad** es la t√©cnica que nos permite resolver este problema. Es como si tom√°ramos todas esas variables econ√≥micas y las \"destil√°ramos\" para obtener la esencia. El **An√°lisis de Componentes Principales (PCA)** es nuestro \"alambique\".\n",
        "\n",
        "PCA nos permitir√° crear nuevas variables (componentes principales) que son una combinaci√≥n de las originales. Por ejemplo, podr√≠amos descubrir que:\n",
        "* **Componente Principal 1 (CP1):** Agrupa fuertemente el PIB, la inversi√≥n y el bajo desempleo. Podr√≠amos interpretarlo como un **\"√çndice de Crecimiento y Prosperidad\"**.\n",
        "* **Componente Principal 2 (CP2):** Agrupa la estabilidad de precios (baja inflaci√≥n) y una deuda p√∫blica controlada. Podr√≠amos llamarlo **\"√çndice de Estabilidad Macroecon√≥mica\"**.\n",
        "\n",
        "De esta forma, pasamos de 50 variables dif√≠ciles de manejar a solo 2 o 3 √≠ndices sint√©ticos que capturan la mayor parte de la informaci√≥n relevante y que s√≠ podemos usar para comparar, visualizar y analizar a los pa√≠ses de manera efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advertencias\n",
        "\n",
        "- PCA solo se puede utilizar para variables num√©ricas, ya que se base en el c√°lculo de la covarianza y la varianza.\n",
        "- Las variables categ√≥ricas se pueden incluir si primero se transforman a n√∫meros con One-Hot Encoding y Ordinal Encoding.\n",
        "- Obligatoriamente, las variables se deben estandarizar o ajustar a valores entre cero y uno."
      ],
      "metadata": {
        "id": "qBI9PeGXG8H_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5aToHiaYBPK"
      },
      "source": [
        "## ¬øC√≥mo funciona PCA?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# ID del video: https://www.youtube.com/watch?v=x-7BHjMA15M\n",
        "YouTubeVideo('x-7BHjMA15M', width=800, height=450)"
      ],
      "metadata": {
        "id": "Y1a-QJxxw9GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de saltar a un complejo dataset, entendamos la magia de PCA con un ejemplo de juguete de solo **2 variables**, con datos que generaremos nosotros mismos."
      ],
      "metadata": {
        "id": "FkAbso-SxavC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "-es5JStPdyRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generar Datos Correlacionados\n",
        "np.random.seed(42) # Para reproducibilidad\n",
        "# Creamos 100 puntajes de matem√°ticas (media 70, std dev 10)\n",
        "puntajes_mat = np.random.normal(70, 10, 100)\n",
        "# Creamos puntajes de f√≠sica correlacionados con matem√°ticas + algo de ruido\n",
        "puntajes_fis = puntajes_mat * 0.8 + np.random.normal(0, 5, 100) + 10 # El +10 ajusta la media\n",
        "\n",
        "# Combinamos en una matriz X (100 filas, 2 columnas)\n",
        "X_simple = np.vstack((puntajes_mat, puntajes_fis)).T\n",
        "\n",
        "print(\"Primeros 5 puntajes:\")\n",
        "X_simple[:5,:]"
      ],
      "metadata": {
        "id": "B4v_iJKvd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-YS9s28YBPL"
      },
      "outputs": [],
      "source": [
        "# 2. Graficar los Datos Originales\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(X_simple[:, 0], X_simple[:, 1], alpha=0.7)\n",
        "plt.title('Puntajes Inventados de Estudiantes (Correlacionados)')\n",
        "plt.xlabel('Puntaje Matem√°ticas')\n",
        "plt.ylabel('Puntaje F√≠sica')\n",
        "plt.axis('equal') # Asegura que la escala de los ejes sea la misma\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_simple = pd.DataFrame(X_simple, columns=['Matem√°ticas', 'F√≠sica'])\n",
        "corr = df_simple.corr().round(2)\n",
        "sns.heatmap(corr, annot=True, cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JU-x8C20bc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPrwV0DVYBPM"
      },
      "source": [
        "**El Problema:**\n",
        "\n",
        "Como esper√°bamos, las variables `Puntaje_Matem√°ticas` y `Puntaje_F√≠sica` est√°n **altamente correlacionadas**. Los estudiantes buenos en una tienden a serlo en la otra. El gr√°fico muestra una nube de puntos alargada y diagonal. Tenemos dos dimensiones (eje X = Matem√°ticas, eje Y = F√≠sica), pero la informaci√≥n es **redundante**. La mayor parte de la \"acci√≥n\" (la varianza) ocurre a lo largo de esa l√≠nea diagonal.\n",
        "\n",
        "**La Soluci√≥n de PCA:**\n",
        "\n",
        "PCA **rotar√° los ejes** para encontrar una nueva perspectiva que describa mejor los datos. Lo hace encontrando las **direcciones de m√°xima varianza**.\n",
        "\n",
        "1.  **Centrar los datos:** PCA funciona sobre datos centrados (media cero). Aunque `StandardScaler` lo hace autom√°ticamente, lo haremos manualmente aqu√≠ para entenderlo mejor.\n",
        "2.  **Aplicar PCA:** Usaremos `sklearn` para encontrar los componentes.\n",
        "3.  **Visualizar los Componentes:** Graficaremos los vectores de los componentes sobre los datos originales."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Centrar los datos (restar la media de cada columna)\n",
        "X_centrado = X_simple - X_simple.mean(axis=0)"
      ],
      "metadata": {
        "id": "ZNMHBtr9b7go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Aplicar PCA\n",
        "pca_simple = PCA()\n",
        "pca_simple.fit(X_centrado)"
      ],
      "metadata": {
        "id": "LphjCXwxetyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDvW62U_YBPN"
      },
      "outputs": [],
      "source": [
        "# Los componentes son los 'eigenvectors'\n",
        "print(\"Direcci√≥n de los Componentes Principales (vectores propios):\")\n",
        "print(pca_simple.components_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¬øQu√© es esto?** Esta matriz te dice la direcci√≥n de los nuevos ejes (Componentes Principales) en relaci√≥n con los ejes originales (Matem√°ticas y F√≠sica). Cada fila representa un componente principal (CP). Los n√∫meros en la fila indican c√≥mo contribuye cada variable original a ese componente.\n",
        "\n",
        "**Fila 1 (CP1):** [0.753, 0.657]\n",
        "\n",
        "- Este es el vector que define la direcci√≥n del primer componente principal (CP1), el eje naranja en nuestro gr√°fico.\n",
        "\n",
        "- Nos dice que para moverte una unidad a lo largo de este nuevo eje CP1, necesitas moverte 0.753 unidades en la direcci√≥n del eje original de Matem√°ticas Y 0.657 unidades en la direcci√≥n del eje original de F√≠sica.\n",
        "\n",
        "- **Interpretaci√≥n:** Como ambos n√∫meros son positivos y de magnitud similar, significa que CP1 representa una combinaci√≥n ponderada de ambas habilidades. Un puntaje alto en CP1 significa ser bueno tanto en Matem√°ticas como en F√≠sica. Por eso lo llamamos \"√çndice General de Aptitud Cient√≠fica\".\n",
        "\n",
        "**Fila 2 (CP2):** [-0.657, 0.753]\n",
        "\n",
        "- Este es el vector que define la direcci√≥n del segundo componente principal (CP2), el eje verde, que es perpendicular al CP1.\n",
        "\n",
        "- Para moverte una unidad a lo largo de CP2, necesitas moverte -0.657 en Matem√°ticas (hacia la izquierda) y +0.753 en F√≠sica (hacia arriba).\n",
        "\n",
        "- **Interpretaci√≥n:** Este componente captura la diferencia relativa entre las habilidades. Un puntaje alto en CP2 podr√≠a significar ser relativamente mejor en F√≠sica que en Matem√°ticas, y viceversa. Sin embargo, como veremos, este componente es mucho menos importante."
      ],
      "metadata": {
        "id": "Z2Rj7s7fu4_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La varianza explicada por cada uno de los componentes son los 'eigenvalues' (o explained_variance_)\n",
        "print(\"\\nVarianza explicada por cada componente (valores propios):\")\n",
        "print(pca_simple.explained_variance_)"
      ],
      "metadata": {
        "id": "0UEMsTP_vQ--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¬øQu√© es esto?** Estos n√∫meros (los eigenvalues o valores propios) te dicen cu√°nta varianza (informaci√≥n, dispersi√≥n) de los datos originales es capturada por cada componente principal. Son la \"longitud\" efectiva de los nuevos ejes en t√©rminos de informaci√≥n.\n",
        "\n",
        "**CP1:** 134.9\n",
        "\n",
        "- El primer componente captura una gran cantidad de la dispersi√≥n total de los puntos.\n",
        "\n",
        "**CP2:** 13.6\n",
        "\n",
        "- El segundo componente captura mucha menos dispersi√≥n.\n",
        "\n",
        "- **Interpretaci√≥n:** La diferencia enorme entre estos dos valores confirma lo que vimos en el gr√°fico: la mayor parte de la \"acci√≥n\" (la variabilidad de los puntajes) ocurre a lo largo de la direcci√≥n del CP1. El CP2 captura variaciones mucho menores."
      ],
      "metadata": {
        "id": "sKyh0XJmvS6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPorcentaje de varianza explicada:\")\n",
        "print(pca_simple.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "VbNqddnrvSda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¬øQu√© es esto?** Es simplemente la varianza explicada de cada componente (el punto anterior) expresada como un porcentaje de la varianza total. Se calcula dividiendo la varianza de cada componente entre la suma de todas las varianzas (134.9 / (134.9 + 13.6) y 13.6 / (134.9 + 13.6)).\n",
        "\n",
        "**CP1:** 0.908 (o 90.8%)\n",
        "\n",
        "- El primer componente principal captura el 90.8% de toda la informaci√≥n (varianza) presente en los puntajes originales de Matem√°ticas y F√≠sica ü§Ø\n",
        "\n",
        "**CP2:** 0.092 (o 9.2%)\n",
        "\n",
        "- El segundo componente solo captura el 9.2% restante.\n",
        "\n",
        "- **Interpretaci√≥n y decisi√≥n:** Este resultado es la justificaci√≥n para la reducci√≥n de dimensionalidad. Como el CP1 explica una porci√≥n tan abrumadoramente grande de la informaci√≥n, podemos decidir descartar el CP2 con una p√©rdida m√≠nima de informaci√≥n (solo el 9.2%). Pasamos de representar a cada estudiante con 2 n√∫meros (Mate, F√≠sica) a representarlo con 1 solo n√∫mero (su proyecci√≥n sobre el eje CP1), nuestro \"√çndice General de Aptitud Cient√≠fica\".\n",
        "\n",
        "Hemos simplificado el problema exitosamente."
      ],
      "metadata": {
        "id": "OjDO8chHvcdr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjBjt0ILYBPO"
      },
      "outputs": [],
      "source": [
        "# 3. Visualizar los Componentes\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(X_centrado[:, 0], X_centrado[:, 1], alpha=0.7, label='Datos Centrados')\n",
        "\n",
        "# Graficamos los vectores de los componentes\n",
        "for i, (comp, var) in enumerate(zip(pca_simple.components_, pca_simple.explained_variance_)):\n",
        "    # Multiplicamos el componente (vector unitario) por la ra√≠z de la varianza (desviaci√≥n est√°ndar)\n",
        "    # para darle una longitud proporcional a la informaci√≥n que captura. Usamos un factor (3) para que sea m√°s visible.\n",
        "    comp = comp * np.sqrt(var) * 3\n",
        "    plt.arrow(0, 0, comp[0], comp[1],\n",
        "              head_width=3, head_length=5, fc=f'C{i+1}', ec=f'C{i+1}', linewidth=2,\n",
        "              label=f'CP{i+1} ({pca_simple.explained_variance_ratio_[i]*100:.1f}%)')\n",
        "\n",
        "plt.title('Datos Centrados y Componentes Principales')\n",
        "plt.xlabel('Matem√°ticas (centrado)')\n",
        "plt.ylabel('F√≠sica (centrado)')\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3ysdhuYBPO"
      },
      "source": [
        "**Interpretaci√≥n del gr√°fico y los resultados:**\n",
        "\n",
        "- Los **puntos azules** son nuestros datos originales, pero centrados en el origen (0,0).\n",
        "- El **vector naranja (CP1)** apunta exactamente en la direcci√≥n donde los datos est√°n m√°s dispersos (la direcci√≥n de la diagonal). PCA nos dice que este componente explica ¬°el **90.8%** de la varianza total!\n",
        "- El **vector verde (CP2)** es perpendicular al CP1 y apunta en la direcci√≥n de la varianza restante (solo el **9.2%**).\n",
        "\n",
        "**La Reducci√≥n:**\n",
        "\n",
        "Como el CP1 captura casi toda la informaci√≥n, podemos **descartar el CP2**.\n",
        "\n",
        "Hemos **reducido la dimensionalidad de 2D a 1D**. En lugar de tener dos puntajes (`Puntaje_Matem√°ticas`, `Puntaje_F√≠sica`), ahora podemos representar a cada estudiante con un solo valor: su proyecci√≥n sobre el eje CP1. Este nuevo valor podr√≠a interpretarse como un **\"√çndice General de Aptitud Cient√≠fica\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de PCA para Visualizaci√≥n y Segmentaci√≥n"
      ],
      "metadata": {
        "id": "FZWFdi8Wy6Er"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqHoZif1YBPP"
      },
      "source": [
        "Ahora veremos un ejemplo cl√°sico. El dataset de \"Vinos\" contiene los resultados de un an√°lisis qu√≠mico de **13 constituyentes** (alcohol, acidez, magnesio, etc.) para **3 tipos diferentes de vino** cultivados en la misma regi√≥n de Italia.\n",
        "\n",
        "**El Problema:** ¬øPodemos visualizar si estos 3 tipos de vino son qu√≠micamente distintos? Graficar en 13 dimensiones es imposible.\n",
        "\n",
        "**El Objetivo:** Usaremos PCA para reducir estas 13 dimensiones a solo 2 y as√≠ poder graficar los 3 tipos de vino en un plano 2D para ver si se separan y se diferencian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X84yTi9e_dDM"
      },
      "source": [
        "### Importar librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jvnUtZiYBPQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mejorar visualizaci√≥n de dataframes y gr√°ficos"
      ],
      "metadata": {
        "id": "jHE2ZLJuEPIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Que muestre todas las columnas\n",
        "pd.options.display.max_columns = None\n",
        "# En los dataframes, mostrar los float con dos decimales\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "# Configuraciones para una mejor visualizaci√≥n\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "id": "72TA8V1fETCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset de vinos desde sklearn\n",
        "wine_data = load_wine()\n",
        "X_wine = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "y_wine = wine_data.target # Esta es nuestra variable objetivo (tipo de vino 0, 1 o 2)"
      ],
      "metadata": {
        "id": "gokKZSwrhciC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_wine.head()"
      ],
      "metadata": {
        "id": "O6FC4yvMhjDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_wine"
      ],
      "metadata": {
        "id": "hkJvovsxhks_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEICmxc_YBPR"
      },
      "outputs": [],
      "source": [
        "print(\"dimensiones de las caracter√≠sticas: \", X_wine.shape)\n",
        "print(\"\\nNombre de las 13 variables qu√≠micas:\")\n",
        "print(X_wine.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr = X_wine.corr().round(2)\n",
        "sns.heatmap(corr, annot=True, cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZXppmgRId0yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B9hX7eBYBPS"
      },
      "source": [
        "### Estandarizaci√≥n de los Datos\n",
        "\n",
        "Como vimos, este paso es **cr√≠tico**. Las 13 variables qu√≠micas tienen escalas totalmente diferentes, por lo que debemos estandarizarlas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqr44Qq2YBPS"
      },
      "outputs": [],
      "source": [
        "scaler_wine = StandardScaler()\n",
        "X_wine_scaled = scaler_wine.fit_transform(X_wine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GE8dDvNYBPT"
      },
      "source": [
        "### Aplicaci√≥n de PCA y Selecci√≥n de Componentes\n",
        "\n",
        "Vamos a aplicar PCA y ver cu√°nta varianza explican los componentes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_wine = PCA()\n",
        "pca_wine.fit(X_wine_scaled)"
      ],
      "metadata": {
        "id": "Zl1BdxdKoiKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGxXqYUUYBPT"
      },
      "outputs": [],
      "source": [
        "# Graficamos la varianza explicada acumulada\n",
        "varianza_acumulada_wine = np.cumsum(pca_wine.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(range(1, len(varianza_acumulada_wine) + 1), varianza_acumulada_wine, marker='o', linestyle='--')\n",
        "plt.title('Varianza Explicada Acumulada (Dataset Vinos)', fontsize=16)\n",
        "plt.xlabel('N√∫mero de Componentes Principales', fontsize=12)\n",
        "plt.ylabel('Varianza Acumulada Explicada', fontsize=12)\n",
        "plt.axhline(y=0.8, color='g', linestyle='-', label='80% de Varianza')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ExxYJ1PYBPU"
      },
      "source": [
        "**Interpretaci√≥n del Gr√°fico:**\n",
        "\n",
        "Los dos primeros componentes (CP1 y CP2) explican **m√°s del 55%** de la varianza total. Si bien no es un umbral alt√≠simo como 90%, para un objetivo de **visualizaci√≥n** en 2D, est√° bien. Nos quedaremos solo con 2 componentes.\n",
        "\n",
        "Vamos a re-aplicar PCA pidiendo solo 2 componentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9YFus6bYBPU"
      },
      "outputs": [],
      "source": [
        "pca_wine_2 = PCA(n_components=2)\n",
        "X_wine_pca = pca_wine_2.fit_transform(X_wine_scaled)\n",
        "\n",
        "print(\"Dimensiones del dataset original:\", X_wine_scaled.shape)\n",
        "print(\"Dimensiones del dataset reducido:\", X_wine_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Varianza explicada\n",
        "pca_wine_2.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "8inLoSQ9fFNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Varianza explicada acumulada\n",
        "pca_wine_2.explained_variance_ratio_.cumsum()"
      ],
      "metadata": {
        "id": "nq2VEfchfeJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**En el gr√°fico de varianza acumulada, ¬øc√≥mo decido el n√∫mero exacto de componentes?**\n",
        "\n",
        "No hay una regla dura y fija, es m√°s un criterio de negocio o de investigaci√≥n. La elecci√≥n implica un trade-off entre la simplicidad del modelo y la cantidad de informaci√≥n que retenemos. Las gu√≠as m√°s comunes son:\n",
        "\n",
        "- **El M√©todo del Codo:** Buscas en el gr√°fico el punto donde a√±adir un nuevo componente ya no aporta un gran aumento en la varianza explicada. Es el \"codo\" de la curva.\n",
        "\n",
        "- **Umbral de Varianza Acumulada:** La regla m√°s usada en la pr√°ctica. Se decide un umbral, por ejemplo, 'quiero explicar al menos el 90% de la varianza'. Y se eligen los componentes necesarios para alcanzarlo. Para an√°lisis exploratorios r√°pidos, un 80% puede ser suficiente. Para modelos que ir√°n a producci√≥n, se suele exigir 95% o incluso 99%.\n",
        "\n",
        "- **Interpretabilidad:** A veces, eliges un n√∫mero de componentes que, aunque no lleguen a un umbral alto, son muy f√°ciles de interpretar econ√≥micamente. Por ejemplo, si con 3 componentes explicas el 75% de la varianza y puedes llamarlos 'Crecimiento', 'Estabilidad' y 'Capital Humano', esa simplicidad puede ser m√°s valiosa que tener 8 componentes que expliquen el 95% pero sean ininteligibles.\""
      ],
      "metadata": {
        "id": "-9wccMBxFe_p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEM_mbPiYBPV"
      },
      "source": [
        "### 1.4 Visualizaci√≥n de la Segmentaci√≥n de Vinos\n",
        "\n",
        "Ahora, vamos a graficar cada vino usando sus nuevos dos ejes (CP1 y CP2) y los colorearemos por su tipo de vino real (la variable `y_wine`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos los resultados en un nuevo dataframe\n",
        "df_wine_pca = pd.DataFrame(X_wine_pca, columns=['CP1', 'CP2'])\n",
        "df_wine_pca['Tipo_Vino'] = y_wine\n",
        "df_wine_pca.head()"
      ],
      "metadata": {
        "id": "3EfW9zgrpSDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHUkPDmyYBPV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='CP1', y='CP2', data=df_wine_pca, hue='Tipo_Vino', palette='viridis', s=100, alpha=0.8)\n",
        "plt.title('Segmentaci√≥n de Vinos usando los 2 Componentes Principales', fontsize=16)\n",
        "plt.xlabel('Componente Principal 1', fontsize=12)\n",
        "plt.ylabel('Componente Principal 2', fontsize=12)\n",
        "plt.legend(title='Tipo de Vino')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcSooFGvYBPV"
      },
      "source": [
        "**Conclusi√≥n de la Visualizaci√≥n:**\n",
        "\n",
        "PCA logr√≥ tomar las 13 variables qu√≠micas y proyectarlas en 2 dimensiones de tal forma que los tres tipos de vino (0, 1 y 2) **quedan casi perfectamente separados**.\n",
        "\n",
        "Esto nos demuestra que la composici√≥n qu√≠mica de los vinos es, en efecto, distinta, y que PCA fue capaz de encontrar los \"ejes\" (componentes) que mejor demuestran esta separaci√≥n. Ahora, cualquier modelo de clasificaci√≥n que use CP1 y CP2 ser√° extremadamente efectivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzDgnD3sYBPW"
      },
      "source": [
        "### Interpretaci√≥n de los Loadings (¬øQu√© significan CP1 y CP2?)\n",
        "\n",
        "Ya vimos que los componentes separan los vinos, pero... ¬øqu√© son? ¬øQu√© caracter√≠stica qu√≠mica est√°n midiendo? Para esto, analizamos los **loadings**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_wine_loadings = pd.DataFrame(pca_wine_2.components_,\n",
        "                                columns=wine_data.feature_names,\n",
        "                                index=['CP1', 'CP2'])\n",
        "df_wine_loadings"
      ],
      "metadata": {
        "id": "s6RNbs3Hpn23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfs2i1_JYBPW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "sns.heatmap(df_wine_loadings, annot=True, cmap='viridis', fmt='.2f')\n",
        "plt.title('Mapa de Calor de los Loadings (Dataset Vinos)', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaulZHUqYBPW"
      },
      "source": [
        "**Interpretaci√≥n üçá:**\n",
        "\n",
        "Miremos qu√© variables tienen los pesos (loadings) m√°s altos en cada componente:\n",
        "\n",
        "* **Componente Principal 1 (CP1):**\n",
        "    * Tiene cargas **altas y positivas** en `flavanoids` (0.42), `total_phenols` (0.39) y `od280/od315_of_diluted_wines` (0.38). Estos son **antioxidantes** que vienen de la c√°scara de la uva y dan estructura y color al vino.\n",
        "    * Tiene cargas **negativas** en `nonflavanoid_phenols` (-0.3), `malic_acid` (-0.25) y `alcalinity_of_ash` (-0.24).\n",
        "    * **Interpretaci√≥n:** El CP1 parece ser un **\"√çndice de Estructura y Madurez\"**. Un valor alto en CP1 (hacia la derecha en el gr√°fico) representa vinos con muchos fenoles y flavonoides (t√≠pico de vinos tintos robustos). Vemos que los vinos tipo 0 est√°n a la derecha, y los tipo 1 y 2 a la izquierda.\n",
        "\n",
        "* **Componente Principal 2 (CP2):**\n",
        "    * Tiene cargas **altas y positivas** en `color_intensity` (0.53), `alcohol` (0.48) y `proline` (0.36).\n",
        "    * Tiene cargas **altas y negativas** en `hue` (-0.28) y `od280/od315_of_diluted_wines` (-0.16).\n",
        "    * **Interpretaci√≥n:** El CP2 parece ser un **\"√çndice de Graduaci√≥n Alcoh√≥lica y Color\"**. Un valor alto en CP2 (hacia arriba en el gr√°fico) representa vinos con m√°s alcohol y color. Vemos que este eje es el que mejor separa los vinos tipo 1 (abajo) de los vinos tipo 2 (arriba).\n",
        "\n",
        "**Conclusi√≥n:** Hemos reducido 13 variables qu√≠micas a 2 √≠ndices interpretables que segmentan perfectamente el mercado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicaci√≥n: Clustering con K-Means sobre Componentes Principales\n",
        "\n",
        "Hemos visto que PCA ayuda a visualizar la separaci√≥n natural de los vinos. ¬øPodr√≠a un algoritmo de clustering como K-Means **descubrir estos grupos por s√≠ mismo**, usando solo los componentes principales?\n",
        "\n",
        "Como vimos con el dataset de casas, K-Means funciona mucho mejor en espacios de baja dimensionalidad. Apliqu√©moslo a nuestros 2 componentes del vino."
      ],
      "metadata": {
        "id": "SSol0QNbJqG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "l_W6BUblKH5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos KMeans pidiendo 3 clusters (sabemos que hay 3 tipos de vino)\n",
        "kmeans_wine = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "\n",
        "# Ajustamos y predecimos sobre los datos PCA (X_wine_pca tiene CP1 y CP2)\n",
        "df_wine_pca['Cluster_KMeans'] = kmeans_wine.fit_predict(X_wine_pca)\n",
        "\n",
        "# Visualizamos los clusters encontrados por K-Means\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='CP1', y='CP2', data=df_wine_pca, hue='Cluster_KMeans', palette='viridis', s=100, alpha=0.8)\n",
        "plt.title('Clusters de Vinos Encontrados por K-Means usando PCA', fontsize=16)\n",
        "plt.xlabel('CP1: √çndice de Estructura Fen√≥lica y Madurez', fontsize=12)\n",
        "plt.ylabel('CP2: √çndice de Intensidad Alcoh√≥lica y Crom√°tica', fontsize=12)\n",
        "plt.legend(title='Cluster K-Means')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pV7Zx6ZoJwVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparaci√≥n y Conclusi√≥n:**\n",
        "\n",
        "Comparen este gr√°fico con el anterior coloreado por `Tipo_Vino`. K-Means, utilizando solo las coordenadas CP1 y CP2, ha sido capaz de **recrear casi perfectamente la separaci√≥n original** de los tres tipos de vino. Los colores de los clusters pueden no coincidir exactamente con los n√∫meros originales (K-Means asigna 0, 1, 2 arbitrariamente), pero la **agrupaci√≥n es casi la misma**.\n",
        "\n",
        "Esto demuestra c√≥mo PCA, al reducir la dimensionalidad y el ruido, crea una representaci√≥n de los datos donde los grupos subyacentes se vuelven mucho m√°s evidentes y f√°ciles de detectar para algoritmos como K-Means. Es un excelente ejemplo de **PCA como preprocesamiento para clustering**."
      ],
      "metadata": {
        "id": "SxoUc3b_J47r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ventajas de utilizar PCA\n",
        "\n",
        "- **Interpretabilidad:** Es imposible explicarle a un gerente o a un ministro qu√© significa el coeficiente de 50 variables diferentes. Pero es muy poderoso decirle: 'Hemos creado un √çndice de Dinamismo Econ√≥mico (nuestro CP1), y por cada punto que un pa√≠s mejora en este √≠ndice, la inversi√≥n extranjera aumenta en X%'. PCA nos ayuda a contar una historia m√°s clara.\n",
        "\n",
        "- **Reducir la Multicolinealidad:** En econometr√≠a, sabemos que la multicolinealidad (alta correlaci√≥n entre variables predictoras) infla los errores est√°ndar de nuestros coeficientes y hace que el modelo sea inestable. Como los componentes principales son, por construcci√≥n, no correlacionados entre s√≠, eliminamos este problema de ra√≠z.\n",
        "\n",
        "- **Eficiencia Computacional:** Entrenar un modelo con 500 variables puede ser lento y costoso. Si PCA nos permite obtener un 95% de la misma informaci√≥n con solo 30 componentes, el proceso es mucho m√°s r√°pido y eficiente. Es como optimizar los recursos de c√≥mputo.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "upaMPzskGhsf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gG56ZMgYBPm"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "La **reducci√≥n de dimensionalidad con PCA** es una herramienta poderosa para el economista y el cient√≠fico de datos, ya que nos permite:\n",
        "\n",
        "1.  **Combatir la Maldici√≥n de la Dimensionalidad:** Simplificamos modelos complejos, reducimos el riesgo de overfitting y mejoramos la eficiencia computacional.\n",
        "2.  **Crear Variables Sint√©ticas Interpretables:** Pasamos de un conjunto de variables correlacionadas a un nuevo conjunto de \"√≠ndices\" (los componentes) que resumen la informaci√≥n y tienen un significado econ√≥mico o de negocio.\n",
        "3.  **Mejorar la Visualizaci√≥n:** Nos permite explorar la estructura oculta de nuestros datos en 2D o 3D, revelando patrones que ser√≠an invisibles de otra manera (Ej. Vinos).\n",
        "4.  **Actuar como un paso de Ingenier√≠a de Caracter√≠sticas:**\n",
        "    - **Para Clustering:** Permite que algoritmos como K-Means funcionen correctamente al reducir el ruido y el espacio dimensional.\n",
        "    - **Para Regresi√≥n Lineal y, en general, Aprendizaje Supervisado:** Resuelve el problema de la multicolinealidad. Se tiene un modelo mucho m√°s parsimonioso (menos variables). Puede, incluso, mejorar la precisi√≥n del modelo al filtrar el ruido."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}